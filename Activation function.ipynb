{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a56d55-5db6-4b8e-bced-67147e14c61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is an activation function in the context of artificial neural networks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c80ca9-6b7f-45bc-bbb8-57dc13796e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans = What is a Neural Network Activation Function?\n",
    "An Activation Function decides whether a neuron should be activated or not.\n",
    "This means that it will decide whether the neuron's input to the network is important\n",
    "or not in the process of prediction using simpler mathematical operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510490ca-6ad9-451b-9ce5-25848fc2f034",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0187afef-6a9b-45cf-8207-38030b7df269",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. What are some common types of activation functions used in neural networks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c092f1-00d8-434b-89d2-2b082200d4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans = 1.Linear or Identity Activation Function.\n",
    "2.Non-linear Activation Function.\n",
    "3.Sigmoid or Logistic Activation Function.\n",
    "4.Tanh or hyperbolic tangent Activation Function.\n",
    "5.ReLU (Rectified Linear Unit) Activation Function.\n",
    "6.Leaky ReLU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d719c95-44b8-44fc-8ba6-ac879169073a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a779e638-6b05-4108-a87a-a2ae31a9b9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. How do activation functions affect the training process and performance of a neural network?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8aefa3-df07-478b-a093-be0056b4307d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans = Activation functions are a critical part of the design of a neural network.\n",
    "The choice of activation function in the hidden layer will control how well the network \n",
    "model learns the training dataset.\n",
    "The choice of activation function in the output layer will define the \n",
    "type of predictions the model can make."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5482e6-264e-4075-879a-dfb9af6e4557",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9782c06e-59a6-401f-b5b4-cf1dddb6585f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. How does the sigmoid activation function work? What are its advantages and disadvantages?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c833d1d-709b-4735-86e9-ca1f89fc4708",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans = With 1 and 0, it makes a clear prediction.\n",
    "Another advantage of this function is that when used with\n",
    "as in the linear function, it returns a value in the range of (0,1).\n",
    "As a result, the activation value does not disappear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e3a273-f644-41dd-b164-4ba642c79011",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c49bb7-2927-4129-bc62-2ab7eb005bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5.What is the rectified linear unit (ReLU) activation function? How does it differ from the sigmoid function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f4c464-7026-492a-9a05-a1f8c53dc25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans = The model trained with ReLU converged quickly and thus takes much less time when\n",
    "compared to models trained on the Sigmoid function.\n",
    "We can clearly see overfitting in the model trained with ReLU.\n",
    "This is due to the quick convergence. \n",
    "The model performance is significantly better when trained with ReLU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db06ebf9-7414-45d4-a728-b35386fb3ffb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6933c4bc-14ae-42a0-94d7-bc455a66dd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. What are the benefits of using the ReLU activation function over the sigmoid function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b007cc-4084-4351-93ae-11e3eefeb34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans = Efficiency: ReLu is faster to compute than the sigmoid function,\n",
    "and its derivative is faster to compute.\n",
    "This makes a significant difference to training and inference time \n",
    "for neural networks: only a constant factor, but constants can matter.\n",
    "Simplicity: ReLu is simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ff3c3d-63bd-4e7c-89ec-a41ab187128c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79226a3b-41e0-4723-9396-dab1202711cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7. Explain the concept of \"leaky ReLU\" and how it addresses the vanishing gradient problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3abfbfe-a7e5-4022-8d1e-11d2209c7173",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans = Because ReLU is known for vanishing gradients, since any values less than zero are mapped to zero.\n",
    "This is true regardless of the number of layers.\n",
    "LeakyReLU on the other hand, maps the values less than zeros to a very small positive number. \n",
    "This prevents vanishing gradient from occurring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649e56af-098a-4995-81d2-5faab0bb8498",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9a345a-25ab-4a67-a443-e44ef81c7c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q8. What is the purpose of the softmax activation function? When is it commonly used?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50e6e0f-4ebb-4e83-8851-994c8cb0bc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans = The softmax function is used as the activation function in the output layer of neural network \n",
    "models that predict a multinomial probability distribution. \n",
    "That is, softmax is used as the activation function for multi-class\n",
    "classification problems where class membership is required on more than two class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284eaf48-559c-491c-afc7-44dec7c9f70f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323ca7bb-6604-469f-9e7a-8dfc0691c193",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q9. What is the hyperbolic tangent (tanh) activation function? How does it compare to the sigmoid function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299dd28f-c072-4173-acfe-82a9440ffae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans = We observe that the gradient of tanh is four times greater \n",
    "than the gradient of the sigmoid function.\n",
    "This means that using the tanh activation function results in \n",
    "higher values of gradient during training\n",
    "and higher updates in the weights of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c558a2fc-f7d3-4408-b89e-ef9cce784d7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
